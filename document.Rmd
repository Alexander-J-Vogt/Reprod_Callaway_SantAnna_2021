---
title: "Difference-in-Differece with Various Treatment Timing and Multiple Period"
output:
  html_document:
    df_print: paged
---

# 1. Difference-in-Difference with Multiple Time Periods



$$
Y_{i, t}=\underbrace{\alpha_t+\alpha_i}_{Time \ \&\ Unit \ FE}+ \underbrace{\sum_{e=-K}^{-2} \delta_e^{\text {anticip }} \cdot D_{i, t}^e}_{Pre-trend \ \ Effects}+ \underbrace{\sum_{e=0}^L \beta_e \cdot D_{i, t}^e+v_{i, t}}_{Treatment \ Effects}
$$

# 3. Code-Implementation of the Methodology

```{r message=FALSE, warning=FALSE}
# In this line Remove existing environment in order to make sure that 
rm(list = ls(all = TRUE))

# set seed for replication
set.seed(1234)

# libraries
library(tidyverse)
library(data.table)
library(haven)
library(DRDID)
# library(did)
# library(BMisc)
library(plm)
library(lmtest)

```

## 3.1 Calculating the aggregated average treatment effects

In the following, the two-step procedure of CS (2021) is implemented in form of two separate functions. In the first function `calculating_agg_att`, the partially and overall aggregated average treatment effects (Agg. ATE) are calculated by first calculating the group-time average treatment effects $ATT(g,t)$ and than using the aggregation methodology of CS (2021).  The second function `calculate_att_se`, calculates the standard error of each aggregated average treatment effects with multiplier bootstrapping.

The function `calculating_agg_att` is the main function of this term paper and needs several arguments in order to calculate Agg. ATT. The structure of the function is inspired by the code application of CS (2021).  The core argument is the method-argument, where the user can choose between the simple-weighted ATE, group-specific ATE, calendar time effects, and event-study effects with balanced and unbalanced event-time. The balanced-argument is needed for the case of performing the balanced event study and requires a number indicating for how many periods a group should have already been treated. Moreover, the unconditional_ind-argument decides on whether to use the unconditional parallel trend assumption (PTA) or the conditional PTA, the later one is the default option. If the conditional PTA is used, pre-treatment covariates should be given to the formula-argument. Additionally, the function requires a balanced panel data set and the name of the time indicator variable, the group-indicator variable, the outcome variable, the individual identification variable and the treatment-indicator variable.

```{r}

# Unconditional DiD-estimator  -----
unconditional_att <- function(outcome_post, outcome_pre, treatment) {
# The Unconditional_att-function calculates the unconditional ATT in the 
# calculating_agg_att-function by first estimating the treatment effect for the 
# pre- and post-treatment outcome separately.
  results_post <- lm(outcome_post ~ treatment)
  results_pre  <- lm(outcome_pre ~ treatment)
  coef_post <- coef(results_post)[2]
  coef_pre  <- coef(results_pre)[2]
  # Then, the difference between the two treatment effects is taken, which 
  # yields the ATT.  
  att <- coef_post - coef_pre
  
  # Saving the results in list
  results <- list()
  results <- list(ATT = att)
}

# spec_formula_log <- ~ -1 + lwhite_pop + lpoverty + lpop_1000s + lmedian_income_1000s + leduc

calculating_agg_att <- function(data,
                                year_input,
                                group_input,
                                outcome_input,
                                id_input,
                                treatment,
                                formula,
                                unconditional_ind = FALSE,
                                method,
                                balanced = NULL
                                ) {
  
  # 1. Standardize variables & basic calculations
  # In this section, the arguments given to the function are re-defined in order
  # to allow a universal usage of the function and the basic variables needed for 
  # the further procedure are defined.
  
  # The variables are re-defined and standardized in order to guarantee universal
  # application of the function.
  data_original <- data
  data_original <- data_original |>
    rename(year    = !!sym(year_input),
           group   = !!sym(group_input),
           outcome = !!sym(outcome_input),
           id      = !!sym(id_input)
           )
  unconditional <- unconditional_ind
  covariate_formula <- formula
  
  # data_original <- data_original |>
  #   rename(year    = date_y, #!!sym(year_input),
  #          group   = group, #!!sym(group_input),
  #          outcome = lnEmp, #!!sym(outcome_input),
  #          id      = county_id# !!sym(id_input)
  #   )
  #   unconditional <- FALSE
  #   covariate_formula <- ~ -1 + lwhite_pop + lpoverty + lpop_1000s + lmedian_income_1000s + leduc
  # Determining the unique time periods, groups and the number of unique values 
  # of the id-variable . Thereby, the timelist is a tx1 vector and grouplist 
  # a gx1 vector.
  timelist <- unique(data_original$year)
  grouplist <- sort(unique(data_original$group))
  grouplist <- grouplist[grouplist != 0]
  n_unique <- length(unique(data_original$id))
  
  # Here, an empty data frame is setup for later saving the ATT(g,t) 
  num_of_cases <- (length(timelist) - 1) * length(grouplist)
  attgt.df <- as.data.frame(matrix(NA, nrow = num_of_cases, ncol = 3))
  colnames(attgt.df) <- c("attgt", "group", "year")
  # att.gt.ls <- list()
  
  # The number-variable is needed for the Loop, which calculates the different
  # ATT(g,t) and is needed for saving the ATT(g,t) in the right row of the
  # data frame.
  number <- 1
  
  # 2. The double loop calculates the ATT(g,t), i.e., the disaggregated ATE.
  # Therefore, the first loop iterates over all different groups and the inner 
  # loop iterates over the available periods. Hence, the inner loop calculates 
  # all the ATT(g,t) of one group.
  for (g in grouplist) {
     for (t in timelist) {
      
      # The data needs to be assigned for every iteration newly, in order to have 
      # the full dataset available as later in the code, the data gets subset.
      data <- data_original
      
      # The ATT(g,t) is calculated for two periods for a treated group g and 
      # uses the "never-treated" units as control group. Therefore, (A) a 
      # pre-treatment period (or reference year) needs to be determinant and (B) 
      # the data needs to include two years and the treatment and control group.
      
      # (A) The choice of reference year depends on whether the period of the current 
      # loop is laying in the pre- or post-treatment period. If the current period
      # of the loop is laying in the post-treatment period, then the reference year 
      # is equal to the first pre-treatment period of a group. Otherwise, the 
      # reference year is set equal to the current period of the loop iteration, 
      # which allows to estimate pseudo- ATT(g,t) like the pre-treatment 
      # estimators in the dynamic TWFE regression.
      if (t >= g) {
        reference_year <- g - 1
      } else {
        reference_year <- t
      }
      
      # (B) After having determined the reference year, the dataset can be subsetted
      # given the reference year and the period after the current loop-period.
      data <- subset(data, year %in% c(reference_year, t + 1))
      
      # (C) In the next step, the two-year dataset gets further restricted by only
      # including the treatment group of the current outer-loop and the
      # "never-treated" group. Therefore, a index-indicator is needed to
      # get a dataset, which only includes these two groups.
      data$g_ <- ifelse(data$group == g, 1, 0) 
      data$c_ <- ifelse(data$group == 0, 1, 0)
      index_gc <- (data$g_ == 1) | (data$c_ == 1)
      # The actual subsetting of the restricted data.
      data_sel <- data[index_gc,]

      # Create treatment indicator based on the current group in the outer-loop.
      data_sel <- data_sel |> mutate(treat = ifelse(group == g, 1, 0))
      
      # This if-condition makes sure that the subsetted dataset includes exactely 
      # two periods and jumps into the next loop if it is not the case.
      if (length(unique(data$year)) == 2) {
      
      # (D) The functions for calculating the conditional and unconditional ATT(g,t)'s,
      # need data in wide format in which the outcome variable is separately available as
      # outcome in the pre- and post-treatment period. Thus, rows of the wide dataset 
      # is equal to the number of unique observations/id-variable. (The code is
      # an implementation of BMisc::panel2cs2. s.h. Callaway (2024))
      data_wide <- arrange(data_sel, id, year)
      data_wide$out_post <- data_wide$outcome
      data_wide$out_pre <- shift(data_wide$outcome, 1)
      data_wide$index <- ifelse(data_wide$year == reference_year, 1, 0)  
      data_wide <- data_wide[data_wide$index == 0,]
      
      } else {
        next
      }

      # (F) The final step is to compute the conditional and unconditional ATT(g,t)
      # for group at period t. Thereby, the user of the function can simply choose
      # between these two options by simply setting the argument "unconditional"
      # respectively to the preferred estimator.
      
      # Creating a matrix with all values of the pre-treatment covariates
      covariates <- model.matrix(covariate_formula, data_wide)
      
      if (unconditional == TRUE) {
      # The first option is to calculate the ATT(g,t) with the PTA holding unconditionally.
      # The estimation has already been  explained in detail in the beginning of this
      # section and is a classical canonical DiD ATE.
      att <- unconditional_att(outcome_post = data_wide$out_post,
                               outcome_pre  = data_wide$out_pre,
                               treatment    = data_wide$treat)
        
      } else {
      
      # The second option is to calculate the ATT(g,t) with the PTA holding conditional
      # to pre-treatment covariates following the non-parametric identified doubly-robust (DR)
      # DiD estimation of Sant’Anna & Zhao (2020). The DR DiD combines the outcome regression 
      # estimator (OR) of Heckman et al. (1997) and inverse probability weighting 
      # estimator (IPW) of Abadie (2005). Thereby, Sant'Anna & Zhao (2022) stress
      # out that the OR relies on a correctly modeled outcome evolution of the 
      # control group conditional on covariates, while the IPW only relies on the
      # correctly modeled probability of belonging to group g. The DR DiD estimator 
      # is a combination of the OR and IPW and thus has the advantage 
      # that it yields a correct estimator, if either the outcome evolution or 
      # the probabilities of belonging to a group is correctly estimated.
      att <- DRDID::drdid_panel(y1 = data_wide$out_post,
                                y0 = data_wide$out_pre,
                                D  = data_wide$treat,
                                covariates = covariates,
                                inffunc    = FALSE,
                                boot       = FALSE)
      }
      
      # All ATT(g,t) effects are saved in this data frame and is the final
      # result of this loop.
      attgt.df[number, 1] <- att$ATT 
      attgt.df[number, 2] <- g
      attgt.df[number, 3] <- t + 1
      
      # Number is increased after every iteration by one in order to save the 
      # values of the next iteration in the next row of the data frame.
      number <- number + 1
    }
  }
  

# 3. Aggregated ATT(g,t) according to CS (2021) ----

## 3.0 Preparation of probabilities & index ----
  
  # Set up a copy of the original dataset as the original dataset is later needed
  # for the event study effects.
  data <- data_original
  
  # After having calculated the ATT(g,t) for each group in each period, the 
  # weights for aggregating the different ATT(g,t) need to be calculated.
  # Therefore, the following loop determines the probability of belonging to a
  # treated group g.
  # First, creating an empty dataset for saving the weights in it.
  weights <- as.data.frame(matrix(NA, nrow = length(grouplist), ncol = 3))
  colnames(weights) <- c("group", "size", "probs")
  
  # Secondly, the weights of each group are calculated
  for (i in seq_along(grouplist)) {
    # The group for which the weight is calculated, is getting selected.
    g <- grouplist[i]
    
    # Then, the size of the subgroup g is calculated and divided by the total
    # size of the unique observations in the panel dataset.
    weights[i, 1] <- g
    weights[i, 2] <- nrow(data[data$group == g & data$year == g, ])
    weights[i, 3] <- weights[i, 2] / n_unique
  }
  
  # In the last step, the probabilities of each group are getting merged 
  # with the data frame, which contains the ATT(g,t). This dataset is playing 
  # a key role in the following section for the calculation of the Agg. ATT. 
  attgt_probs_df <- merge(attgt.df, weights, by.x = "group" ) 
  
  # The index_post variable is a index for the Prob-ATT-df, which contains the 
  # row of each ATT(g,t) for each group in the post-treatment period.
  index_post   <- which(attgt_probs_df$year >= attgt_probs_df$group)
  

## 3.1 Simple Weighted Average of ATT(g,t) -------------------------------------
  
  if (method == "simple_att") {
  
    # The simple-weighted average of ATT(g,t) is a measure to evaluate the overall 
    # ATE of the treatment. In order to calculate the measure, all the ATT(g,t) 
    # of all groups in the post-treatment period are weighted with the respective 
    # probability of belonging to the group g. Then, all the weighted ATT(g,t) 
    # are summed up and divided by kappa. Kappa has the purpose to normalize the 
    # weights and is the sum of all used probabilities over all time periods and 
    # groups. 
    
    # This line selects all the relevant ATT(g,t) in the post-treatment period.
    aggte_simple <- attgt_probs_df[index_post, ]
    
    # Than the  sum of probabilities corresponding to ATT(g,t) in the
    # post-treatment period over all groups and periods is calculated.
    kappa <- sum(aggte_simple$probs)
    
    # The simple-weighted ATT is calculated according to the description above.
    simple_att_est <- round(sum(aggte_simple$attgt * aggte_simple$probs) / kappa, 4)
    
    # This line saves the results.
    results <- list(overall_att = simple_att_est)
    return(results)
  }

## 3.2 Group-Time ATT(g,t) ----
  
  if (method == "group_specific_att") {
    
    # 1) Partially Agg. ATE 
    # The group-specific ATE is of relevance for researchers if they want to 
    # evaluate the heterogeneity of a policy with respect to treatment timing.
    # Thereby, the group-specific ATE is calculated by taking the mean of all 
    # available ATT(g,t) of a group g according to Sant'Anna (2022).
    
    # The first step is to select the ATT(g,t) of each group in the post-treatment 
    # period from the data frame with all ATT(g,t) and the corresponding weights
    # of each group.
    gte_df <- attgt_probs_df[index_post, ]
    
    # An empty data frame is created, in which the group-specific ATE are saved.
    gte_results <- data.frame(matrix(NA, nrow = length(grouplist), ncol = 2))
    colnames(gte_results) <- c("time", "coef")
    
    # Than, the second step is to calculate the group-specific ATE by taking the 
    # mean of all ATT(g,t) of a group g as above described. In each loop iteration,
    # the group-specific ATE is calculated for another group.
    for (i in seq_along(grouplist)) {
      g <- grouplist[i]
      gte_results[i, "time"] <- g
      gte_results[i, "coef" ]  <- round(mean(gte_df[gte_df$group == g, "attgt"]), 4)
    }
    
    # 2) Overall Agg. ATE
    # The calculation of the overall ATE of the group-specific ATE follows the
    # the same strategy of simple weighted average of ATT. Thus, the sum of
    # weighted group-specific ATT is taken. Than, this sum is divided by the sum of
    # of all group weights for normalization purposes. Note that the sum with which
    # is divide equals the kappa of the simple-weighted average ÁTT.
    agg_gte_results <- round(sum(gte_results$coef * weights$probs) / sum(weights$probs), 4)
    
    # This line saves the results.
    results <- list(partial_att = gte_results, overall_att = agg_gte_results)
    return(results)
  }

## 3.3 Calendar Time Effects ----
  if (method == "calendar_att") {
    
    # 1) Partially Agg. ATE
    # Calendar time effects are of relevance if the question arises on how large 
    # the ATE after a certain number of periods is (CS, 2021). 
    # The calendar time effects are calculated by taking the sum of ATT(g,t) with 
    # the corresponding weights by calendar time, e.g., by year 2006. The first 
    # calendar time effect is calculated for the year in which the first group
    # is treated.
    
    # Selecting the ATT(g,t) of each group in the post-treatment like in the
    # other steps before.
    aggte_ct <- attgt_probs_df[index_post,]

    # The calendar time effects are calculated by looping of all unique calendar 
    # and, hence, the unique calendar times need to be determined. These are
    # all years, which are in the post-treatment period. 
    calendar_timelist <- unique(aggte_ct$year)
    
    # Than, an empty data frame is created, in which the calendar time effects are 
    # going to be save.
    cte_results <- as.data.frame(matrix(NA, nrow = length(calendar_timelist), ncol = 2))
    colnames(cte_results) <- c("time", "coef")
    
    # In this loop the calendar effects for each calendar time are calculated.
    for (i in seq_along(calendar_timelist)) {
      # Selecting the post-treatment year for which the calendar time effects 
      # is calculated in the current iteration.
      calendar_time <- calendar_timelist[i]
      df <- aggte_ct[aggte_ct$year == calendar_time,]
      # Calculating the calendar time effects by weighting ATT(g,t) with the
      # corresponding weights and divided by the sum of all used weights
      group_prob <- df$probs / sum(df$probs)
      cte_results[i, "time"] <- calendar_time
      cte_results[i, "coef"]   <- round(sum(df$attgt * group_prob), 4)
    }
    
    # 2) Overall Agg. ATT 
    # The overall calendar time effects is calculated by taking the mean of all
    # calendar time effects. 
    agg_att_ct <- round(mean(cte_results$coef), 4)
    
    # Save final calendar time effects
    results <- list(partial_att = cte_results, overall_att = agg_att_ct)
    return(results)
  }


## 3.4 Event Study Design ----
   if (method %in% c("unbalanced_eventstudy", "balanced_eventstudy")) {
      
    # In this section the event study effects is calculated for the balanced and
    # unbalanced event time. Therefore, the interest lays on the heterogeneity of 
    # treatment effects with respect to exposure to the treatment. In the first 
    # step, the event-time needs to be calculated by taking the difference 
    # between year of the ATT(g,t) and the group of a ATT(g,t).  
    attgt_et <- attgt_probs_df
    attgt_et$eventtime <- attgt_et$year - attgt_et$group
    
    # Than all pre-treatment event times are exuded as the interest lays on the 
    # exposure to the treatment.
    attgt_et <- attgt_et[attgt_et$eventtime >= 0, ]

### 3.4.1 Event Study Effect ----
    if (method == "unbalanced_eventstudy") {
      
      # 1) Partial Agg. ATE
      # The event study effects without balancing the event time measures the
      # effect of a treatment e periods after a treatment. Therefore, the weighted
      # sum of all group-time ATT of a event-time e is taken and divided by the 
      # sum of all weights of groups in event-time e.
      
      # The calculation of the event study effect requires a variable with the 
      # unique set of event-time. Than, the loop can be used to calculate the 
      # event study effects for each event-time e.
      eventime_timelist <- unique(attgt_et$eventtime)
      
      # Creating an empty dataset to safe the event study effects of each
      # event-time e
      et_results <- as.data.frame(matrix(NA, nrow = length(eventime_timelist), ncol = 2))
      colnames(et_results) <- c("time", "coef")
      
      # The loop to calculate the event effect effects
      for (i in seq_along(eventime_timelist)) {
        # Selecting the event-time e of the current iteration
        eventime_time <- eventime_timelist[i]
        df         <- attgt_et[attgt_et$eventtime == eventime_time,]
        # Calculating the event study effect according to the aforementioned 
        # description.
        group_prob <- df$probs / sum(df$probs)
        et_results[i, "time"]   <- eventime_time
        et_results[i, "coef"]   <- round(sum(df$attgt * group_prob), 4)
      }
      
      # 2) Overall Agg. ATE
      # The overall event study effect is calculated by taking the mean of all
      # event study effects time effects.
      agg_et <- round(mean(et_results$coef), 4)
      
      # Save final calendar time effects
      results <- list(partial_att = et_results, overall_att = agg_et)
      return(results)
    } # End of unbalanced event-study effects if-clause

## 3.4.2 Eventstudy with balanced group ---------------------------------------
    if (method == "balanced_eventstudy" & !is.null(balanced)) {
      
      # 1) Partially Agg. ATT
      # The event study effect with groups being balanced with respect to the
      # event-time e condition the event study effect on a group being observed
      # at least for a certain amount of time according to CS (2021). Thereby,
      # the calculation of the "balanced event study effect" is identical to the 
      # "unbalanced event study effect".
      
      # (A) First, the indicator on how many periods a group should have been 
      # observed is re-defined as it is an argument of the function.
      balance_groups <- balanced
      
      # (B) Secondly, all the groups, which were treated at least for a certain 
      # amount of periods, must be determined.Therefore, the number of 
      # post-treatment periods is calculated by iterating over the vector with 
      # the unique treatment groups.
      grouplist_et <- unique(attgt_et$group)
      att_per_group <- sapply(grouplist_et, function(g){
        # Create a subset of the ATT(g,t) with only group g being represented.
        df <- attgt_et[attgt_et$group == g,]
        # Than the number of post-treatment period is summed up.
        n  <- nrow(df)
        n
      })
      
      # Create a data frame, which contains the information on the number
      # of post-treatment periods.
      att_per_group <- data.frame(grouplist_et, att_per_group)
      colnames(att_per_group) <- c("eventtime", "abs_nr_att")

      # The loop determines whether a group was  exposed to the treatment for 
      # the required length of "balanced_group". After the loop, groups_to_exclude 
      # contains the groups, which were not observed for required treatment length.
      groups_to_exclude <- c()
      for ( i in seq_along(att_per_group$eventtime) ) {
        if ( att_per_group[i, 2] < balance_groups + 1 ) {
          groups_to_exclude[i] <- att_per_group[i, 1]
        } else {
          next
        }
      }
      
      # In the last step, the "balanced" data frame of ATT(g,t) is obtained by 
      # excluding the determined groups in groups_to_exclude().
      groups_to_exclude <- groups_to_exclude[!is.na(groups_to_exclude)]
      attgt_et_bal <- attgt_et[!attgt_et$group %in% groups_to_exclude,]
      
      # (C) Now, the event study effect can be calculated in the same way as the 
      # in the unbalanced event study effect section.
      eventime_timelist <- unique(attgt_et_bal$eventtime)
      att_et_bal <- as.data.frame(matrix(NA, nrow = length(eventime_timelist), ncol = 2))
      colnames(att_et_bal) <- c("time", "coef")
        
      for (i in seq_along(eventime_timelist)) {
        eventime_time <- eventime_timelist[i]
        df         <- attgt_et[attgt_et_bal$eventtime == eventime_time,]
        group_prob <- df$probs / sum(df$probs)
        att_et_bal[i, "time"]   <- eventime_time
        att_et_bal[i, "coef"]   <- round(sum(df$attgt * group_prob), 4)
      }  
      
      # Only the event study effects, which lay in the minimum requirement
      # of how long a group must have been observed. are shown.
      et_bal_results <- att_et_bal[att_et_bal$time < balance_groups + 1,]
      
      # 2) Overall Agg. ATT
      # Calculate the aggregated event study effects by taking the mean of all
      # event study effects in the different event-time.
      agg_bal_et <- round(mean(et_bal_results$coef), 4)
      
      # Save results on balanced event study effects
      results <- list(partial_att = et_bal_results, overall_att = agg_bal_et)
      return(results)
    } 
  } 
}
```

## 3.2 Calculating the Standard Errors

The function `calculate_att_se` is the final step of implementing the methodology of CS (2021). The outcome of the function is a list of the partially and overall Agg. ATE of the choosen method and the corresponding standard errors, which are calculated by using the multiplier bootstrap proposed in CS (2021). Thereby, it is important to mention that the multiplier bootstrap in CS (2021) is used to bootstrap the influence function of an ATE, while in this term paper it used to calculate a distribution of the partial and ATT from which the distribution is calculated.

```{r}
calculate_att_se <- function(data = qwi,
                             year_input = "date_y",
                             group_input = "group",
                             outcome_input = "lnEmp",
                             id_input = "county_id",
                             treatment = treated,
                             formula = spec_formula,
                             unconditional_ind = FALSE,
                             method = "group_att",
                             balanced = FALSE) {
  
  # These variables are redefined for universal usage in the function. 
  data <- data
  data_for_b <- data |>
    rename(id = !!sym(id_input)) |>
    as.data.frame()
  method <- method
  # The number of iteration is determined here.
  iter <- 10
  b_res_overall <- list()
  b_res_partial <- list()
  # This loop is the implementation of the multiplier bootstrap. The core
  # difference to the standard bootstrapping is that there are us no resembling
  # but only a fraction of the overall total population size is used for
  # calculating the ATE.
  for (i in 1:iter) {
  
    # The selection of the subset of the total sample is determined by the 
    # Bernoulli Variates according to CS (2021), Thereby, a vector with values
    # of the binomial distribution is created. The vector has the same length
    # as the unique observation of the dataset. The probability for which 1 are
    # drawn from is equal to the following term:
    kappa <- ( sqrt(5) + 1 ) / 2
    p <- kappa / sqrt(5)
    n_row <- length(data_for_b$id)
    # Creation of the above mentioned vector.
    bernoulli_variates <- rbinom(n_row, 1, p)
    
    # In these three lines the subset of the total sample is drawn, which is then
    # used to calculate the partial and overall ATE with the method of choice.
    county <- unique(data_for_b$id)
    county <- county[bernoulli_variates == 1]
    b_data <- data_for_b |> filter(id %in% county)
    b_est <- calculating_agg_att(data = b_data,
                                 year_input = year_input,
                                 group_input = group_input,
                                 outcome_input = outcome_input,
                                 id_input = "id",
                                 treatment = treated,
                                 formula = formula,
                                 unconditional_ind = unconditional_ind,
                                 method = method,
                                 balanced = balanced)
    
    # The results of the ATE with only a sub sample are saved here, and these
    # variables contain the distribution of every partial and overall ATE.
    if (method != "simple_att")  b_res_partial[[i]] <- t(as.matrix(b_est$partial_att$coef))
    b_res_overall[[i]] <- round(b_est$overall_att, 4)
  }
  
  # The standard error of the partial and overall ATE are calculated from their 
  # respective distribution of ATE, but before the list of which contains the
  # ATT must be collapsed in order to yield a data frame.
  b_res_partial <- map_dfr(b_res_partial, as.data.frame)
  se_partial <- apply(b_res_partial, 2, sd)
  b_res_overall <- unlist(b_res_overall)
  se_overall <- round(sd(b_res_overall), 4)
  
  # Calculating the ATT with the full sample.
  est <- calculating_agg_att(data = data,
                             year_input = year_input,
                             group_input = group_input,
                             outcome_input = outcome_input,
                             id_input = id_input,
                             treatment = treated,
                             formula = spec_formula,
                             unconditional_ind = unconditional_ind,
                             method = method,
                             balanced = balanced)
  
  # The results of the ATT and the corresponding standard erros are
  # saved as the outcome of this function.
  est$partial_att$b_se <- round(se_partial, 4)
  est$overall_att <- cbind(est$overall_att, se_overall)
  colnames(est$overall_att) <- c("coef", "b_se")
  
  return(est)
}
  
```

```{r include=FALSE}
calculate_various_specifications <-  function(data) {
                
  method_opt <- c("simple_att", "group_specific_att", "calendar_att", 
                  "unbalanced_eventstudy", "balanced_eventstudy")

  for ( m in method_opt ) {
    paste0("Method: ", m)
    assign(paste0("c_est_", m), calculate_att_se(data = data,
                                               year_input = "date_y",
                                               group_input = "group",
                                               outcome_input = "lnEmp",
                                               id_input = "county_id",
                                               treatment = treated,
                                               formula = spec_formula,
                                               unconditional_ind = FALSE,
                                               method = m,
                                               balanced = 1))
    
  }

  for ( m in method_opt ) {
    paste0("Method: ", m)
    assign(paste0("u_est_", m), calculate_att_se(data = data,
                                                    year_input = "date_y",
                                                    group_input = "group",
                                                    outcome_input = "lnEmp",
                                                    id_input = "county_id",
                                                    treatment = treated,
                                                    formula = spec_formula,
                                                    unconditional_ind = TRUE,
                                                    method = m,
                                                    balanced = 1))
    
  }
    
   c_twfe <- plm(lnEmp ~ -1 + post_treat + treated + post_treat * treated + 
                    lwhite_pop + lpoverty + lpop_1000s + lmedian_income_1000s + 
                    leduc, data = data, index = c("county_id", "date_y"), model = "within") 
   
   c_twfe_df <- as.data.frame(matrix(NA, nrow = 1, ncol = 2))
   colnames(c_twfe_df) <- c("coef", "se")
   c_twfe_df[1,1] <- round(coef(c_twfe)[2], 4)
   c_twfe_df[1,2] <- round(lmtest::coeftest(c_twfe, vcov = vcovHC)[2,2], 4)
   
    
   u_twfe <- plm(lnEmp ~ -1 + post_treat + treated + post_treat * treated 
                       , data = data, index = c("county_id", "date_y"), model = "within")
   
   u_twfe_df <- as.data.frame(matrix(NA, nrow = 1, ncol = 2))
   colnames(u_twfe_df) <- c("coef", "se")
   u_twfe_df[1,1] <- round(coef(u_twfe)[2], 4)
   u_twfe_df[1,2] <- round(lmtest::coeftest(u_twfe, vcov = vcovHC)[2,2], 4)

   
   cond <- list(twfe = c_twfe_df, 
                simple = c_est_simple_att, 
                group = c_est_group_specific_att,
                calendar = c_est_calendar_att, 
                eventstudy_bal = c_est_balanced_eventstudy,
                eventstudy_unbal = c_est_unbalanced_eventstudy)
   
   uncond <- list(twfe = c_twfe_df, 
                  simple = u_est_simple_att, 
                  group = u_est_group_specific_att,
                  calendar = u_est_calendar_att, 
                  eventstudy_bal = u_est_balanced_eventstudy,
                  eventstudy_unbal = u_est_unbalanced_eventstudy) 
   
   results <- list(c_results = cond,
                   u_results = uncond)
   
}

## Calculating all methods except balanced eventstudy
calculate_various_specifications_west <-function(data) {
  
  method_opt <- c("simple_att", "group_specific_att", "calendar_att", 
                  "unbalanced_eventstudy")
  
  for ( m in method_opt ) {
    paste0("Method: ", m)
    assign(paste0("c_est_", m), calculate_att_se(data = data,
                                                 year_input = "date_y",
                                                 group_input = "group",
                                                 outcome_input = "lnEmp",
                                                 id_input = "county_id",
                                                 treatment = treated,
                                                 formula = spec_formula,
                                                 unconditional_ind = FALSE,
                                                 method = m,
                                                 balanced = 1))
    
  }
  
  for ( m in method_opt ) {
    paste0("Method: ", m)
    assign(paste0("u_est_", m), calculate_att_se(data = data,
                                                 year_input = "date_y",
                                                 group_input = "group",
                                                 outcome_input = "lnEmp",
                                                 id_input = "county_id",
                                                 treatment = treated,
                                                 formula = spec_formula,
                                                 unconditional_ind = TRUE,
                                                 method = m,
                                                 balanced = 1))
    
  }
  
  c_twfe <- plm(lnEmp ~ -1 + post_treat + treated + post_treat * treated + 
                  lwhite_pop + lpoverty + lpop_1000s + lmedian_income_1000s + 
                  leduc, data = data, index = c("county_id", "date_y"), model = "within") 
  
  c_twfe_df <- as.data.frame(matrix(NA, nrow = 1, ncol = 2))
  colnames(c_twfe_df) <- c("coef", "se")
  c_twfe_df[1,1] <- round(coef(c_twfe)[2], 4)
  c_twfe_df[1,2] <- round(lmtest::coeftest(c_twfe, vcov = vcovHC)[2,2], 4)
  
  
  u_twfe <- plm(lnEmp ~ -1 + post_treat + treated + post_treat * treated 
                , data = data, index = c("county_id", "date_y"), model = "within")
  
  u_twfe_df <- as.data.frame(matrix(NA, nrow = 1, ncol = 2))
  colnames(u_twfe_df) <- c("coef", "se")
  u_twfe_df[1,1] <- round(coef(u_twfe)[2], 4)
  u_twfe_df[1,2] <- round(lmtest::coeftest(u_twfe, vcov = vcovHC)[2,2], 4)
  
  
  cond <- list(twfe = c_twfe_df, 
               simple = c_est_simple_att, 
               group = c_est_group_specific_att,
               calendar = c_est_calendar_att,
               eventstudy_unbal = c_est_unbalanced_eventstudy)
  
  uncond <- list(twfe = c_twfe_df, 
                 simple = u_est_simple_att, 
                 group = u_est_group_specific_att,
                 calendar = u_est_calendar_att,
                 eventstudy_unbal = u_est_unbalanced_eventstudy) 
  
  results <- list(c_results = cond,
                  u_results = uncond)
  
}
```

# 4. Application of Callaway & Sant'Anna (2021)

Callaway & Sant'Anna (2021) explore the effect of an increase in the minimum wage on teen employment in the United States. In the period between 2001 and 2007, several states increased their minimum wage above the federal minimum wage in different years, while the federal minimum wage stayed constant. This allows CS (2021) to exploit the variation on state-level with respect to minimum wage increases and enables them to calculate ATE. The groups are defined according to the year of the minimum wage increase and all states without any increase are belonging to the "never-treated" group.

In this term paper, the same application of CS (2021) is performed with the implementation presented above. Thereby, the dataset used by CS (2021) is recreated by using the information on teen employment from the public available Quarterly Workforce Indicator (QWI) on county-level and information on the pre-treatment characteristics of counties in the U.S. from the 2000 County and City Databook. The last dataset allows to control for several county-specific characteristics such as county population in 2000, fraction of the white population, share of the population with high-school diploma, the median income in 1997 and share of population below the poverty level. These values are than used in their logarithmized-version for controlling the county-specific trends and, thus, allow to implement the conditional PTA like in CS (2021). Moreover, it is important to mention that the full sample used in this term paper contain 2260 counties, while the dataset of CS (2021) has 2284. Thus, in terms of sample size both dataset are close to each other.

The evaluation of the effect of the minimum wage increase on teen employment is done on the full sample as in CS (2021), which contains all three different treated groups, i.e., group 2004, group 2006 and group 2007. The research is also extended to evaluate the effect on regional-level according to the Census Region defined by the U.S. Census Bureau. The following regions are observed: the Western, Southern and Midwest region. Thereby, all three groups have a different number treatment groups. While the Midwestern region has all three groups, the southern region only contain group 2006 and 2007 and the Western group has only the group 2007.

In the following the results are discussed after the table structure is explained. In "twfe", the TWFE estimator is saved the standard error and in "simple", the simple-weighted average over all ATT(g,t) can be found with the bootstrapped standard error. The table structure follows a simple structure from here on, the partial effect can be found under partial_effects and the overall aggregated ATE in "overall_att". In "group", the group-specific effects can be found and the time represents the treatment group, e.g., row one represent the group-specific effect of group 2004. In "calendar", the calendar time effects can be found, while the column time represents a year in our available period. In "eventstudy_bal" & "eventstudy_unbal, contain the event study effects for each event-time e.

In Table (1), the TWFE effect is a positive but insignificant estimator. A completely different picture can be seen if the simple-weighted average ATT is observed, which suggests a 4.45% decrease in teen employment, due to the minimum wage increase. The group-specific ATT shows that group 2004, the one with the longest post-treatment period, also has the greatest ATT as teen employment decreases by 8.94%. The group-specific effect declines with a shortening post-treatment period. The calendar time effects can be interpreted as what the ATE of all groups is after a period is. Thus, in row 2007, the interpretation would be that teen emplyoment decreases by 4.03% across all group in 2007. In this context, it is important to point out that the ATE are fairly stable over the years. In the balanced event-study, only groups are included, which were observed for more than one period, i.e., group 2007 is excluded. The effect seems to grow with longer exposure to treatment, which can be seen by the increasing negative effect over time for balanced and unbalanced event study effects.

In table (2), the unconditional ATT for the full sample can be observed, Thereby, it is important to recognize that the effect sizes are similar for every treatment effects independent of its nature. This should not be the case because somebody would expect that the ATT's to lay higher, due to the uncontrolled country-specific trends. One reason for the difference could be the data itself, as CS (2021) give little detail on how the data was matched and which variables excately were used. Thus, the collection of the dataset and cleaning of the dataset could be the core reason for this result.

In the next step, the regional effects are discussed on how they differ to the results of the full sample. The discussion on Western region is excluded because states of this region only increased in the year 2007 the minimum wage. Therefore, the effects for each aggregation version is the same because the group was only treated for one year and there is no variation in treatment timing.

In table (3) and (4), the results on the Midwest region can be interpreted and also here the great similarity between the ATT with conditional and unconditional PTA can be observed. The group-specific effects of the Midwest region for the group 2004 and 2006 are nearly only have the magnitude as in the full sample, while the group 2007 has a double as large estimator. The calendar time effects are comparable to the one in the full sample. The event study effects are showing a different pattern compared to the full sample. The event study effects for balanced and unbalanced data are increasing in magnitude more slowly in the Midwest region, which maybe lets somebody conclude that this region maybe have a smaller problem with teen employmnt compared to other regions.

In table (5) and (6), the mixed results for the Soutern region can be interpreted and also here there is great similarity betwenn the ATT with conditional and unconditional PTA. The group-specific effects are a lot smaller for this region or are equal to zero as it is the case for group 2007. The calendar time effects show that the average effect in 2006 is insignicantly positive, while the effect is positive in the year 2007 but has only one-fourth of the size of the effects of the full sample. The event study effects for balanced and unbalanced group in event-time 0 is nearly equal to zero, while the one period after the treatment a similar effect magnitude as in the full sample is observed.

```{r warning=FALSE, include=FALSE}
# Please set here the working directory

# load data
qwi <- read_rds(paste0("./", "01_Data/cs_data.RDS"))
qwi <- data.table(qwi)

# In these lines, the subset of each Census Region is created.
qwi_west <- qwi |> filter(region == "West")
qwi_midwest <- qwi |> filter(region == "Midwest")
qwi_south <- qwi |> filter(region == "South")

# Defining the covariate formula for the ATE holding under conditional ATE
spec_formula <- ~ -1 + lwhite_pop + lpoverty + lpop + lmeduab_income + leduc

# Calc
results_total   <- calculate_various_specifications(qwi)
results_west    <- calculate_various_specifications_west(qwi_west)
results_midwest <- calculate_various_specifications(qwi_midwest)
results_south   <- calculate_various_specifications(qwi_south)
```

```{r echo=FALSE, message=FALSE, paged.print=TRUE}
print("(1) Full sample: Conditional ATE with Doubly-Robust estimators")
print(results_total$c_results)

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print("(2) Full sample: Unconditional ATE")
print(results_total$u_results)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print("(3) Midwest Census Region: Conditional ATE")
print(results_midwest$c_results)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print("(4) Midwest Census Region: Unconditional ATE")
print(results_midwest$c_results)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print("(5) Soutern Census Region: Conditional ATE")
print(results_south$c_results)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print("(6) Southern Census Region: Unconditional ATE")
print(results_south$c_results)
```
